{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "It is specific to the selection of the required features and influencers and the exclusion of the remainder and is selected based on the extent of its association with the director y, and is done via modules feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectPercentile\n",
    "\n",
    "The selectpercentile tool is used to select the most important fitches associated with the output by the percentage given,\n",
    "\n",
    " and the amount of importance is determined in many ways, such as the f_classif tool or chi2 tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape before process is  (569, 30)\n",
      "\n",
      "X Features are \n",
      " ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      " \n",
      "X Shape after process is  (569, 6)\n",
      "\n",
      "Selected Features are :  [False False  True  True False False False False False False False False\n",
      " False  True False False False False False False  True False  True  True\n",
      " False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2 , f_classif \n",
    "#----------------------------------------------------\n",
    "\n",
    "#load breast cancer data\n",
    "\n",
    "BreastData = load_breast_cancer()\n",
    "\n",
    "#X Data\n",
    "X = BreastData.data\n",
    "#print('X Data is \\n' , X[:10])\n",
    "print('X shape before process is ' , X.shape)\n",
    "print('\\nX Features are \\n' , BreastData.feature_names)\n",
    "\n",
    "#y Data\n",
    "y = BreastData.target\n",
    "#print('y Data is \\n' , y[:10])\n",
    "#print('y shape is ' , y.shape)\n",
    "#print('y Columns are \\n' , BreastData.target_names)\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Feature Selection by Percentile\n",
    "#print('Original X Shape is ' , X.shape)\n",
    "FeatureSelection = SelectPercentile(score_func = chi2, percentile=20) # score_func can = f_classif\n",
    "X = FeatureSelection.fit_transform(X, y)\n",
    "\n",
    "#showing X Dimension \n",
    "print(' \\nX Shape after process is ' , X.shape)\n",
    "print('\\nSelected Features are : ' , FeatureSelection.get_support()) # for i(th) feature true for that selected  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "sel = SelectPercentile(score_func = chi2 , percentile = 20).fit_transform(X,y)\n",
    "sel.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False,  True,  True, False,\n",
       "       False, False, False, False, False, False,  True,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False,  True,\n",
       "       False])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X.shape\n",
    "\n",
    "X_new = SelectPercentile(score_func =chi2, percentile=10)\n",
    "X_new.fit(X, y)\n",
    "selected = X_new.transform(X)\n",
    "X_new.get_support()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GenericUnivariateSelect\n",
    "is a univariate feature selector that allows you to select features based on a configurable strategy. The strategy can be one of the following:\n",
    "\n",
    "* percentile: Select the features with the highest scores, where the scores are percentiles of the overall distribution of scores.\n",
    "\n",
    "For example, if you set the percentile to 25, then the 25 features with the highest scores will be selected.\n",
    "\n",
    "* k_best: Select the top k features with the highest scores.\n",
    "\n",
    "* fpr: Select features based on a false positive rate test. This is a more conservative approach to feature selection than percentile or k_best, as it tries to avoid overfitting.\n",
    "\n",
    "* fdr: Select features based on an estimated false discovery rate. This is another conservative approach to feature selection, similar to fpr.\n",
    "\n",
    "* fwe: Select features based on a family-wise error rate. This is the most conservative approach to feature selection, and it is only recommended if you are very concerned about overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X Shape is  (569, 30)\n",
      "\n",
      "X Shape is  (569, 3)\n",
      "\n",
      "Selected Features are :  [False False False  True False False False False False False False False\n",
      " False  True False False False False False False False False False  True\n",
      " False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.feature_selection import chi2 , f_classif \n",
    "#----------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "print('Original X Shape is ' , X.shape)\n",
    "FeatureSelection = GenericUnivariateSelect(score_func= chi2, mode= 'k_best', param=3) # score_func can = f_classif : mode can = percentile,fpr,fdr,fwe \n",
    "X = FeatureSelection.fit_transform(X, y)\n",
    "\n",
    "#showing X Dimension \n",
    "print('\\nX Shape is ' , X.shape)\n",
    "print('\\nSelected Features are : ' , FeatureSelection.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectKBest\n",
    "is a univariate feature selector that selects the k features with the highest scores, \n",
    "\n",
    "according to a user-specified scoring function. The scoring function can be any function that takes two arrays, X and y, and returns a score for each feature.\n",
    "\n",
    "Some popular scoring functions for classification tasks include:\n",
    "\n",
    "* f_classif: The F-statistic for classification.\n",
    "\n",
    "* chi2: The chi-squared statistic for classification.\n",
    "\n",
    "* mutual_info_classif: The mutual information between each feature and the target variable.\n",
    "\n",
    "Some popular scoring functions for regression tasks include:\n",
    "\n",
    "* f_regression: The F-statistic for regression.\n",
    "\n",
    "* mse: The mean squared error.\n",
    "\n",
    "* mae: The mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X Shape is  (569, 3)\n",
      "X Shape is  (569, 3)\n",
      "Selected Features are :  [ True  True  True]\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2 , f_classif \n",
    "#----------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------\n",
    "#Feature Selection by KBest \n",
    "print('Original X Shape is ' , X.shape)\n",
    "FeatureSelection = SelectKBest(score_func= chi2 ,k=3) # score_func can = f_classif \n",
    "X = FeatureSelection.fit_transform(X, y)\n",
    "\n",
    "#showing X Dimension \n",
    "print('X Shape is ' , X.shape)\n",
    "print('Selected Features are : ' , FeatureSelection.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectFromModel\n",
    "that can be used to select features based on the importance weights of a base estimator. \n",
    "\n",
    "The base estimator can be any estimator that has a coef_ or feature_importances_ attribute after fitting.\n",
    "\n",
    "The SelectFromModel class works by first fitting the base estimator to the data. Then,\n",
    "\n",
    " it calculates the importance weights for each feature and selects the features with the highest importance weights. \n",
    " \n",
    " The threshold value for feature selection can be specified using the threshold parameter.\n",
    " \n",
    "  If the importance weight of a feature is below the threshold, then the feature will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "#----------------------------------------------------\n",
    "\n",
    "#Feature Selection by KBest \n",
    "#print('Original X Shape is ' , X.shape)\n",
    "\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "thismodel = LinearRegression()\n",
    "'''\n",
    "\n",
    "FeatureSelection = SelectFromModel(estimator = thismodel, max_features = None) # make sure that thismodel is well-defined  it refer to modal name like LinerRegrassion()\n",
    "X = FeatureSelection.fit_transform(X, y)\n",
    "\n",
    "#showing X Dimension \n",
    "#print('X Shape is ' , X.shape)\n",
    "#print('Selected Features are : ' , FeatureSelection.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True, False, False,  True,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True,  True, False, False, False,\n",
       "        True, False, False])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators = 20)) \n",
    "sel.fit(X,y)\n",
    "selected_features = sel.transform(X)\n",
    "sel.get_support()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
